# -*- coding: utf-8 -*-
"""LWF ASR_3batches_combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rqB0RrKqmi1MtbKRXIqIdyoo3UwVv8_M
"""

!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
import IPython.display as ipd
from tqdm.notebook import tqdm
import random
torch.manual_seed(0)
np.random.seed(0)
random.seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

"""
import urllib.request

url = 'https://www.cse.iitb.ac.in/~pjyothi/cs753/train_list.txt'
urllib.request.urlretrieve(url, 'train_list.txt')
train_indices = open("/content/train_list.txt").read().splitlines()#.readlines()
train_indices = [int(i) for i in train_indices]
#print(train_indices)
url = 'https://www.cse.iitb.ac.in/~pjyothi/cs753/test_list.txt'
urllib.request.urlretrieve(url, 'test_list.txt')
test_indices = open("/content/test_list.txt").read().splitlines()
test_indices = [int(i) for i in test_indices]
#print(test_indices)
"""

from torchaudio.datasets import SPEECHCOMMANDS
import os
class SubsetSC(SPEECHCOMMANDS):
    def __init__(self, subset: str = None):
        super().__init__("./", download=True)

        def load_list(filename):
            filepath = os.path.join(self._path, filename)
            with open(filepath) as fileobj:
                return [os.path.join(self._path, line.strip()) for line in fileobj]

        if subset == "testing":
            self._walker = load_list("testing_list.txt")
            print("Size of original test set:",len(self._walker))
            #self._walker = [self._walker[i] for i in test_indices]
            #print("Size of new test set:",len(self._walker))
        elif subset == "training":
            excludes = load_list("validation_list.txt") + load_list("testing_list.txt")
            excludes = set(excludes)
            self._walker = [w for w in self._walker if w not in excludes]
            print("Size of original train set:",len(self._walker))
            #self._walker = [self._walker[i] for i in train_indices]
            #print("Size of new train set:",len(self._walker))


# Create training and testing split of the data. We do not use validation in this tutorial.
training_list = SubsetSC("training")
testing_list = SubsetSC("testing")

"""Model A: 10 classes"""

import os
classes = os.listdir("/content/SpeechCommands/speech_commands_v0.02/")
classes.remove("LICENSE")
classes.remove("README.md")
classes.remove("_background_noise_")
classes.remove("testing_list.txt")
classes.remove("validation_list.txt")
classes.remove(".DS_Store")
print(classes)
classes_A = classes[:10]
print("Number of classes", len(classes_A))
print(classes_A)
classes_AX = classes[10:20]
print(classes_AX)
classes_X = classes[30:]
classes_AB = classes[:20]
classes_BC = classes[10:30]
classes_AC = classes[:10]
classes_AC.extend(classes[20:30])

import glob
"""
## Read the test list
with open("/content/SpeechCommands/speech_commands_v0.02/testing_list.txt") as testing_f:
  testing_list = [x.strip() for x in testing_f.readlines()]

print(len(testing_list))
testing_list = [testing_list[i] for i in test_indices]
print("Number of testing samples", len(testing_list))
#print("Number of validation samples", len(validation_list))
"""
## Construct a train list
training_list = []

def load_list(filename):
  filepath = os.path.join("/content/SpeechCommands/speech_commands_v0.02/", filename)
  with open(filepath) as fileobj:
    return [os.path.join("/content/SpeechCommands/speech_commands_v0.02/", line.strip()) for line in fileobj]

testing_list = load_list("testing_list.txt")
testing_listA = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_A ]
testing_listAX = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_AX ]
testing_listX = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_X ]
#testing_list = [testing_list[i] for i in test_indices]
print("Number of testing samples in A", len(testing_listA))
n_last_class = int(len(testing_listA)/10)
testing_listAX = random.sample(testing_listAX, 5*n_last_class)
testing_listBC = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_BC ]
testing_listBC = random.sample(testing_listBC, 5*n_last_class)
print("Number of testing samples in AX", len(testing_listAX))

excludes = load_list("testing_list.txt") + load_list("validation_list.txt")
excludes = set(excludes)

for c in classes:
  training_list += glob.glob("/content/SpeechCommands/speech_commands_v0.02/" + c + "/*")

training_list = [w for w in training_list if w not in excludes]
training_listA = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_A ]
training_listAX = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_AX ]
training_listX = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_X ]
print("Number of train samples in A", len(training_listA))
n_last_class = int(len(training_listA)/10)
training_listAX = random.sample(training_listAX, 5*n_last_class) #training the last class on large number of other class samples
training_listBC = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_BC ]
training_listBC = random.sample(training_listBC, 5*n_last_class)
print("Number of train samples in AX", len(training_listAX))

class SpeechDataset(SPEECHCOMMANDS):
  
  def __init__(self, classes, file_list, classes_x, file_listx):
    super().__init__("/content/", download=True)
    
    self.classes = classes
    self.classes_x = classes_x
    # create a map from class name to integer
    self.class_to_int = dict(zip(classes, range(len(classes))))
    classx_range = [len(classes) for i in range(len(classes_x))] #label of "others" classes is len(classes)
    self.classx_to_int = dict(zip(classes_x, classx_range))

    self.class_to_int.update(self.classx_to_int) #merge into class_to_int
    # store the file names
    self.samples = file_list
    self.samples.extend(file_listx)
    #print(self.samples[0])
    
    # store our MFCC transform
    self.mfcc_transform = torchaudio.transforms.MFCC(n_mfcc=12, log_mels=True)
    
  def __len__(self):
    return len(self.samples)
    
  def __getitem__(self,i):
    with torch.no_grad():
      # load a normalized waveform
      waveform,_ = torchaudio.load(self.samples[i], normalization=True)
      
      # if the waveform is too short (less than 1 second) we pad it with zeroes
      if waveform.shape[1] < 16000:
        waveform = F.pad(input=waveform, pad=(0, 16000 - waveform.shape[1]), mode='constant', value=0)
      
      # then, we apply the transform
      mfcc = self.mfcc_transform(waveform).squeeze(0).transpose(0,1)
    
    # get the label from the file name
    label = self.samples[i].split("/")[4]
    
    # return the mfcc coefficient with the sample label
    return mfcc, self.class_to_int[label]

train_setA = SpeechDataset(classes_A, training_listA, classes_X, training_listX)
val_setA =SpeechDataset(classes_A, testing_listA, classes_X, testing_listX)
#train_setX = SpeechDataset(classes_X, training_listX)
#val_setX =SpeechDataset(classes_X, testing_listX)
print(train_setA[5][0].shape)

train_dlA = torch.utils.data.DataLoader(train_setA, batch_size=32, shuffle=True)
val_dlA = torch.utils.data.DataLoader(val_setA, batch_size=32)

#print(next(iter(train_dl)))

class CNNLayerNorm(nn.Module):
    def __init__(self, n_feats):
        super(CNNLayerNorm, self).__init__()
        self.layer_norm = nn.LayerNorm(n_feats)

    def forward(self, x):
        # x (batch, channel, feature, time)
        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)
        x = self.layer_norm(x)
        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)

class SpeechRNN(torch.nn.Module):
   
  def __init__(self,kernel = 3, stride = 1, dropout = 0.1, n_feats = 128):#original dropout = 0.5
    super(SpeechRNN, self).__init__()

    self.conv1 = torch.nn.Conv2d(1, 32, kernel, stride, padding = kernel//2)
    #Introduce residual connection here
    #self.layer_norm1 = CNNLayerNorm(n_feats)
    self.layer_norm1 = nn.BatchNorm2d(32)
    self.dropout1 = nn.Dropout(dropout)
    self.conv2 = torch.nn.Conv2d(32, 32, kernel, stride, padding = kernel//2)
    #self.layer_norm2 = CNNLayerNorm(n_feats)
    self.layer_norm2 = nn.BatchNorm2d(32)
    self.dropout2 = nn.Dropout(dropout)
    self.conv3 = torch.nn.Conv2d(32, 32, kernel, stride, padding = kernel//2)
    #Introduce residual connection here
    #self.layer_norm3 = CNNLayerNorm(n_feats)
    self.layer_norm3 = nn.BatchNorm2d(32)
    self.dropout3 = nn.Dropout(dropout)
    self.conv4 = torch.nn.Conv2d(32, 32, kernel, stride, padding = kernel//2)
    #self.layer_norm4 = CNNLayerNorm(n_feats)
    self.layer_norm4 = nn.BatchNorm2d(32)
    self.dropout4 = nn.Dropout(dropout)
    self.conv5 = torch.nn.Conv2d(32, 32, kernel, stride, padding = kernel//2)
    #self.bn5 = nn.BatchNorm2d(32)
    self.dropout5 = nn.Dropout(dropout)

    self.linear = nn.Linear(972*32,256)

    #self.layer_norm5 = nn.LayerNorm(256)

    self.linear1 = torch.nn.Linear(256, 256)
    #self.dropout6 = nn.Dropout(dropout)
    #self.linear2 = torch.nn.Linear(256, len(classes))
    
  def forward(self, x):
    x = x.unsqueeze(1)
    x = self.conv1(x)
    residual = x
    x = self.layer_norm1(x)
    x = F.leaky_relu(x)
    x = self.dropout1(x)
    x = self.conv2(x)
    x = self.layer_norm2(x)
    x = F.leaky_relu(x)
    x = self.dropout2(x)
    x = self.conv3(x)
    x = x + residual

    residual1 = x
    x = self.layer_norm3(x)
    x = F.leaky_relu(x)
    x = self.dropout3(x)
    x = self.conv4(x)
    x = self.layer_norm4(x)
    x = F.leaky_relu(x)
    x = self.dropout4(x)
    x = self.conv5(x)
    x = x + residual1
    #print(x.size())

    sizes = x.size()
    x = x.view(sizes[0], sizes[1]*sizes[2]*sizes[3])
    #x = x.transpose(1,2)
    #print(x.size())
    x = self.linear(x)

    #x = self.layer_norm5(x)
    x = F.leaky_relu(x)

    x = self.linear1(x)
    x = F.leaky_relu(x)
    #x = self.dropout6(x)
    #x = self.linear2(x)
    #print(x.size())
    
    return x

class ModelA(nn.Module):
  def __init__(self,classes):
    super(ModelA,self).__init__()
    self.speech_model = SpeechRNN()
    self.final_layer = torch.nn.Linear(256, len(classes)+1)
  
  def forward(self,x):
    x = self.speech_model(x)
    x = self.final_layer(x)
    return x

#net = SpeechRNN().cuda()
netA = ModelA(classes = classes_A).cuda()
print(netA)
batch = next(iter(train_dlA))[0]
print(batch.shape)
y = netA(batch.cuda())

print(y.shape)

##RE-RUN THIS CODE TO GET A "NEW" NETWORK

LEARNING_RATE = 0.0005

## Create an instance of our network
netA = ModelA(classes = classes_A).cuda()

# Negative log likelihood loss
criterion = torch.nn.CrossEntropyLoss()

# Adam optimizer
optimizer = torch.optim.Adam(netA.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

from tqdm import tqdm_notebook
## NUMBER OF EPOCHS TO TRAIN
N_EPOCHS = 5

epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []

for e in range(N_EPOCHS):
  
  print("EPOCH:",e)
  
  ### TRAINING LOOP
  running_loss = 0
  running_accuracy = 0
  
  ## Put the network in training mode
  netA.train()
  
  for i, batch in enumerate(tqdm_notebook(train_dlA)):
    
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]
    
    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netA(x)
    
    # Compute the loss
    loss = criterion(y, labels)
    
    # Reset the gradients
    optimizer.zero_grad()
    
    # Compute the gradients
    loss.backward()
    
    # Apply one step of the descent algorithm to update the weights
    optimizer.step()
    
    ## Compute some statistics
    with torch.no_grad():
      running_loss += loss.item()
      running_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Training accuracy:", running_accuracy/float(len(train_setA)),
        "Training loss:", running_loss/float(len(train_setA)))
  
  epoch_loss.append(running_loss/len(train_setA))
  epoch_acc.append(running_accuracy/len(train_setA))
  
  ### VALIDATION LOOP
  ## Put the network in validation mode
  netA.eval()
  
  running_val_loss = 0
  running_val_accuracy = 0

  for i, batch in enumerate(val_dlA):
    
    with torch.no_grad():
      # Get a batch from the dataloader
      x = batch[0]
      labels = batch[1]

      # move the batch to GPU
      x = x.cuda()
      labels = labels.cuda()

      # Compute the network output
      y= netA(x)
      
      # Compute the loss
      loss = criterion(y, labels)
      
      running_val_loss += loss.item()
      running_val_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Validation accuracy:", running_val_accuracy/float(len(val_setA)),
        "Validation loss:", running_val_loss/float(len(val_setA)))
  
  epoch_val_loss.append(running_val_loss/len(val_setA))
  epoch_val_acc.append(running_val_accuracy/len(val_setA))

import numpy  as np
# Create a test dataset instance
test_datasetA = SpeechDataset(classes_A, testing_listA, classes_X, testing_listX)

# Create a DataLoader
test_dlA = torch.utils.data.DataLoader(test_datasetA, batch_size=1)

netA.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlA):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netA(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetA)),
      "Test loss:", test_loss/float(len(test_datasetA)))

print(netA)

torch.save(netA.state_dict(),'modelA.pth')
torch.save(netA,'modelA.pt')

"""Model B: 10 classes"""

classes_B = classes[10:20]
print("Number of classes", len(classes_B))
print(classes_B)

import glob
"""
## Read the test list
with open("/content/SpeechCommands/speech_commands_v0.02/testing_list.txt") as testing_f:
  testing_list = [x.strip() for x in testing_f.readlines()]

print(len(testing_list))
testing_list = [testing_list[i] for i in test_indices]
print("Number of testing samples", len(testing_list))
#print("Number of validation samples", len(validation_list))
"""
## Construct a train list
training_list = []

def load_list(filename):
  filepath = os.path.join("/content/SpeechCommands/speech_commands_v0.02/", filename)
  with open(filepath) as fileobj:
    return [os.path.join("/content/SpeechCommands/speech_commands_v0.02/", line.strip()) for line in fileobj]

testing_list = load_list("testing_list.txt")
print(len(testing_list))
testing_listB = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_B ]
testing_listBX = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_A ]
n_last_class = int(len(testing_listB)/10)
testing_listBX = random.sample(testing_listBX, 5*n_last_class)
testing_listAC = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_AC ]
testing_listAC = random.sample(testing_listAC, 5*n_last_class)
#testing_list = [testing_list[i] for i in test_indices]
print("Number of testing samples", len(testing_listB))

excludes = load_list("testing_list.txt") + load_list("validation_list.txt")
excludes = set(excludes)

for c in classes:
  training_list += glob.glob("/content/SpeechCommands/speech_commands_v0.02/" + c + "/*")

training_list = [w for w in training_list if w not in excludes]
training_listB = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_B ]
training_listBX = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_A ]
n_last_class = int(len(training_listB)/10)
training_listBX = random.sample(training_listBX, 5*n_last_class)
training_listAC = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_AC ]
training_listAC = random.sample(training_listAC, 5*n_last_class)
print(training_listB[0])
print(testing_listB[0])
#training_list = [training_list[i] for i in train_indices]

print("Number of training samples", len(training_listB))

train_setB = SpeechDataset(classes_B, training_listB, classes_X, training_listX)
val_setB =SpeechDataset(classes_B, testing_listB, classes_X, testing_listX)

print(train_setB[5][0].shape)

train_dlB = torch.utils.data.DataLoader(train_setB, batch_size=32, shuffle=True)
val_dlB = torch.utils.data.DataLoader(val_setB, batch_size=32)

#print(next(iter(train_dl)))

class ModelB(nn.Module):
  def __init__(self,classes):
    super(ModelB,self).__init__()
    self.speech_model = SpeechRNN()
    self.final_layer = torch.nn.Linear(256, len(classes)+1)
  
  def forward(self,x):
    x = self.speech_model(x)
    x = self.final_layer(x)
    return x

netB = ModelB(classes = classes_B).cuda()

checkpointA = torch.load('modelA.pt')
pretrainedA_dict = checkpointA.speech_model.state_dict()
netB_dict = netB.speech_model.state_dict()
"""
for key,_ in pretrained_dict.items():
  print(key)
print("/n")
for key,_ in net1_dict.items():
  print(key)
"""
pretrainedA_dict = {k: v for k, v in pretrainedA_dict.items() if k in netB_dict}
netB_dict.update(pretrainedA_dict)
#net1_dict=pretrained_dict
#print(net1_dict)
netB.speech_model.load_state_dict(netB_dict)#, strict=False)

from copy import deepcopy
from torch.autograd import Variable
from torch.nn import functional as F

def lwf(old_model: nn.Module, model:nn.Module,dataset:torch.utils.data.DataLoader):
  loss = 0
  inp = dataset
  #inp = Variable(dataset).cuda()
  #print(inp.shape)
  old_x = old_model(inp)
  x = model(inp)
  #print(x.shape)
  old_x = old_x.view(1,-1)
  x = x.view(1,-1)
  l = torch.dist(old_x,x,p=2)
  #print(l)
  loss+=l.item()
  
  return loss

"""Train model B"""

##RE-RUN THIS CODE TO GET A "NEW" NETWORK

LEARNING_RATE = 0.0005
elambda = 0.0005 #ewc lambda in loss function

# Negative log likelihood loss
criterion = torch.nn.CrossEntropyLoss()

# Adam optimizer
optimizer = torch.optim.Adam(netB.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

from tqdm import tqdm_notebook
## NUMBER OF EPOCHS TO TRAIN
N_EPOCHS = 5

epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []

for e in range(N_EPOCHS):
  
  print("EPOCH:",e)
  
  ### TRAINING LOOP
  running_loss = 0
  running_accuracy = 0
  #lwf_loss = 0

  ## Put the network in training mode
  netB.train()
  
  for i, batch in enumerate(tqdm_notebook(train_dlB)):
    
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]
    
    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netB(x)
    
    lwf_loss = 0
    lwf_loss = lwf(netA.speech_model, netB.speech_model,x)
    
    # Compute the loss
    loss = criterion(y, labels) + elambda*lwf_loss
    
    # Reset the gradients
    optimizer.zero_grad()
    
    # Compute the gradients
    loss.backward()
    
    # Apply one step of the descent algorithm to update the weights
    optimizer.step()
    
    ## Compute some statistics
    with torch.no_grad():
      running_loss += loss.item()
      running_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Training accuracy:", running_accuracy/float(len(train_setB)),
        "Training loss:", running_loss/float(len(train_setB)), "lwf Loss:", lwf_loss)
  
  epoch_loss.append(running_loss/len(train_setB))
  epoch_acc.append(running_accuracy/len(train_setB))
  
  ### VALIDATION LOOP
  ## Put the network in validation mode
  netB.eval()
  
  running_val_loss = 0
  running_val_accuracy = 0

  for i, batch in enumerate(val_dlB):
    
    with torch.no_grad():
      # Get a batch from the dataloader
      x = batch[0]
      labels = batch[1]

      # move the batch to GPU
      x = x.cuda()
      labels = labels.cuda()

      # Compute the network output
      y= netB(x)
      
      # Compute the loss
      loss = criterion(y, labels)
      
      running_val_loss += loss.item()
      running_val_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Validation accuracy:", running_val_accuracy/float(len(val_setB)),
        "Validation loss:", running_val_loss/float(len(val_setB)))
  
  epoch_val_loss.append(running_val_loss/len(val_setB))
  epoch_val_acc.append(running_val_accuracy/len(val_setB))

torch.save(netB, "modelB.pt")

"""Test model B"""

# Create a test dataset instance
test_datasetB = SpeechDataset(classes_B, testing_listB, classes_X, testing_listX)

# Create a DataLoader
test_dlB = torch.utils.data.DataLoader(test_datasetB, batch_size=1)

netB.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlB):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netB(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetB)),
      "Test loss:", test_loss/float(len(test_datasetB)))

"""Test model A"""

checkpointB = torch.load('modelB.pt')
pretrainedB_dict = checkpointB.speech_model.state_dict()
netA_dict = netA.speech_model.state_dict()
"""
for key,_ in pretrained_dict.items():
  print(key)
print("/n")
for key,_ in net1_dict.items():
  print(key)
"""
pretrainedB_dict = {k: v for k, v in pretrainedB_dict.items() if k in netA_dict}
netA_dict.update(pretrainedB_dict)
#net1_dict=pretrained_dict
#print(net1_dict)
netA.speech_model.load_state_dict(netA_dict)#, strict=False)

netA.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlA):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netA(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetA)),
      "Test loss:", test_loss/float(len(test_datasetA)))

"""
Combined A and B testing"""

classes_AB = classes[:20]
testing_list = load_list("testing_list.txt")
testing_listAB = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_AB ]

#######BREAK#######

test_datasetAB = SpeechDataset(classes_AB, testing_listAB, classes_X, testing_listX)

# Create a DataLoader
test_dlAB = torch.utils.data.DataLoader(test_datasetAB, batch_size=1)

netA.eval()
netB.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])
b1 = 10
#CUDA_LAUNCH_BLOCKING = 1
c = 0
for i, batch in enumerate(test_dlA):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    #print(labels[0])
    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()


    """
    for i in range(0,2):
      if (i == 0):
        y1 = netA(x)
        # Compute the loss
        if (y1.max(1)[1][0].cpu() == b1):
          continue
        labelsA = labels
        print('model A', y1.max(1)[1], ' ', labelsA[0])
        if (labelsA[0] > b1):
          labelsA[0] = b1
        loss = criterion(y1, labelsA)
        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y1.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labelsA.cpu().numpy()])

        test_loss += loss.item()
        #print(y1.max(1))
        #print(y2.max(1))
        #break


        test_accuracy += (y1.max(1)[1] == labelsA).sum().item()
        break

      else:
        y2 = netB(x)
        if (labels[0] < b1):
          labels[0] = 20
        loss = criterion(y2, torch.subtract(labels,10))
        print('model B', y2.max(1)[1], ' ', labels[0])
        
        # add 10 to y2 labels!!
        y2.max(1)[1][0] += b1

        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y2.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labels.cpu().numpy()])
        
        test_loss += loss.item()
        
        test_accuracy += (y2.max(1)[1] == labels).sum().item()
      
    #break
"""
    for i in range(0, 2):
      if (i == 0):
        y1 = netB(x)
        # Compute the loss
        if (y1.max(1)[1][0].cpu() == b1):
          continue
        #c += 1
        #print('model B', y1.max(1)[1], ' ', labels[0])
        
        #uncomment following lines if test_dlAB used else comment
        #if (labels[0] < 10):
        #  labels[0] = 20
        #labels[0] -= 10

        loss = criterion(y1, labels)
        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y1.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labelsA.cpu().numpy()])

        test_loss += loss.item()
        #print(y1.max(1))
        #print(y2.max(1))
        #break


        test_accuracy += (y1.max(1)[1] == labels).sum().item()
        break

      else:
        y2 = netA(x)
        if (labels[0] > 10):
          labels[0] = 10
        loss = criterion(y2, labels)
        #print('model A', y2.max(1)[1], ' ', labels[0])
        
        

        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y2.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labels.cpu().numpy()])
        
        test_loss += loss.item()
        
        test_accuracy += (y2.max(1)[1] == labels).sum().item()
      
    #break
print("Test accuracy:", test_accuracy/float(len(test_datasetA)),
      "Test loss:", test_loss/float(len(test_datasetA)))
#print(c)
#print(len(test_datasetA))

"""Model C: 10 classes"""

classes_C =classes[20:30]
print("Number of classes", len(classes_C))
print(classes_C)

## Construct a train list
training_list = []

def load_list(filename):
  filepath = os.path.join("/content/SpeechCommands/speech_commands_v0.02/", filename)
  with open(filepath) as fileobj:
    return [os.path.join("/content/SpeechCommands/speech_commands_v0.02/", line.strip()) for line in fileobj]

testing_list = load_list("testing_list.txt")
print(len(testing_list))
testing_listC = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_C ]
n_last_class = int(len(testing_listC)/10)
testing_listAB = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_AB ]
testing_listAB = random.sample(testing_listAB, 5*n_last_class)
#testing_list = [testing_list[i] for i in test_indices]
print("Number of testing samples", len(testing_listC))

excludes = load_list("testing_list.txt") + load_list("validation_list.txt")
excludes = set(excludes)

for c in classes:
  training_list += glob.glob("/content/SpeechCommands/speech_commands_v0.02/" + c + "/*")

training_list = [w for w in training_list if w not in excludes]
training_listC = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_C ]
n_last_class = int(len(training_listC)/10)
training_listAB = [training_list[i] for i in range(len(training_list)) if training_list[i].split("/")[4] in classes_AB ]
training_listAB = random.sample(training_listAB, 5*n_last_class)
#training_list = [training_list[i] for i in train_indices]

print("Number of training samples", len(training_listC))

train_setC = SpeechDataset(classes_C, training_listC, classes_X, training_listX)
val_setC =SpeechDataset(classes_C, testing_listC, classes_X, training_listX)

print(train_setC[5][0].shape)

train_dlC = torch.utils.data.DataLoader(train_setC, batch_size=32, shuffle=True)
val_dlC = torch.utils.data.DataLoader(val_setC, batch_size=32)

class ModelC(nn.Module):
  def __init__(self,classes):
    super(ModelC,self).__init__()
    self.speech_model = SpeechRNN()
    self.final_layer = torch.nn.Linear(256, len(classes)+1)
  
  def forward(self,x):
    x = self.speech_model(x)
    x = self.final_layer(x)
    return x

netC = ModelC(classes = classes_C).cuda()

checkpointB = torch.load('modelB.pt')
pretrainedB_dict = checkpointB.speech_model.state_dict()
netC_dict = netC.speech_model.state_dict()
"""
for key,_ in pretrained_dict.items():
  print(key)
print("/n")
for key,_ in net1_dict.items():
  print(key)
"""
pretrainedB_dict = {k: v for k, v in pretrainedB_dict.items() if k in netC_dict}
netC_dict.update(pretrainedB_dict)
#net1_dict=pretrained_dict
#print(net1_dict)
netC.speech_model.load_state_dict(netC_dict)#, strict=False)

"""Train Model C"""

#prec,mat = ewc_penalty(checkpointB.speech_model, train_dlB)
##RE-RUN THIS CODE TO GET A "NEW" NETWORK

LEARNING_RATE = 0.0005
elambda = 0.0005 #ewc lambda in loss function

# Negative log likelihood loss
criterion = torch.nn.CrossEntropyLoss()

# Adam optimizer
optimizer = torch.optim.Adam(netC.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

from tqdm import tqdm_notebook
## NUMBER OF EPOCHS TO TRAIN
N_EPOCHS = 5

epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []

for e in range(N_EPOCHS):
  
  print("EPOCH:",e)
  
  ### TRAINING LOOP
  running_loss = 0
  running_accuracy = 0

  ## Put the network in training mode
  netC.train()
  
  for i, batch in enumerate(tqdm_notebook(train_dlC)):
    
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]
    
    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netC(x)
    
    lwf_loss = 0
    lwf_loss = lwf(netB.speech_model, netC.speech_model,x)
    
    # Compute the loss
    loss = criterion(y, labels) + elambda*lwf_loss
    
    # Reset the gradients
    optimizer.zero_grad()
    
    # Compute the gradients
    loss.backward()
    
    # Apply one step of the descent algorithm to update the weights
    optimizer.step()
    
    ## Compute some statistics
    with torch.no_grad():
      running_loss += loss.item()
      running_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Training accuracy:", running_accuracy/float(len(train_setC)),
        "Training loss:", running_loss/float(len(train_setC)), "LWF Loss:", lwf_loss)
  
  epoch_loss.append(running_loss/len(train_setC))
  epoch_acc.append(running_accuracy/len(train_setC))
  
  ### VALIDATION LOOP
  ## Put the network in validation mode
  netC.eval()
  
  running_val_loss = 0
  running_val_accuracy = 0

  for i, batch in enumerate(val_dlC):
    
    with torch.no_grad():
      # Get a batch from the dataloader
      x = batch[0]
      labels = batch[1]

      # move the batch to GPU
      x = x.cuda()
      labels = labels.cuda()

      # Compute the network output
      y= netC(x)
      
      # Compute the loss
      loss = criterion(y, labels)
      
      running_val_loss += loss.item()
      running_val_accuracy += (y.max(1)[1] == labels).sum().item()
    
  print("Validation accuracy:", running_val_accuracy/float(len(val_setC)),
        "Validation loss:", running_val_loss/float(len(val_setC)))
  
  epoch_val_loss.append(running_val_loss/len(val_setC))
  epoch_val_acc.append(running_val_accuracy/len(val_setC))

torch.save(netC, "modelC.pt")

"""Test Model C"""

# Create a test dataset instance
test_datasetC = SpeechDataset(classes_C, testing_listC, classes_X, testing_listX)

# Create a DataLoader
test_dlC = torch.utils.data.DataLoader(test_datasetC, batch_size=64)

netC.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlC):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netC(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetC)),
      "Test loss:", test_loss/float(len(test_datasetC)))

"""Test Model B, A"""

checkpointC = torch.load('modelC.pt')
pretrainedC_dict = checkpointC.speech_model.state_dict()
netB_dict = netB.speech_model.state_dict()
pretrainedC_dict = {k: v for k, v in pretrainedC_dict.items() if k in netB_dict}
netB_dict.update(pretrainedC_dict)
netB.speech_model.load_state_dict(netB_dict)#, strict=False)


netB.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlB):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netB(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetB)),
      "Test loss:", test_loss/float(len(test_datasetB)))

####-- model A test

checkpointC = torch.load('modelC.pt')
pretrainedC_dict = checkpointC.speech_model.state_dict()
netA_dict = netA.speech_model.state_dict()
pretrainedC_dict = {k: v for k, v in pretrainedC_dict.items() if k in netA_dict}
netA_dict.update(pretrainedC_dict)
netA.speech_model.load_state_dict(netA_dict)#, strict=False)


netA.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])

for i, batch in enumerate(test_dlA):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    # Compute the network output
    y = netA(x)

    # Compute the loss
    loss = criterion(y, labels)
    
    ## Store all the predictions an labels for later
    preds = np.hstack([preds, y.max(1)[1].cpu().numpy()])
    y_test = np.hstack([y_test, labels.cpu().numpy()])

    test_loss += loss.item()
    test_accuracy += (y.max(1)[1] == labels).sum().item()

print("Test accuracy:", test_accuracy/float(len(test_datasetA)),
      "Test loss:", test_loss/float(len(test_datasetA)))

"""Combined Testing"""

classes_ABC = classes[:30]
testing_list = load_list("testing_list.txt")
testing_listABC = [testing_list[i] for i in range(len(testing_list)) if testing_list[i].split("/")[4] in classes_ABC ]

#######BREAK#######

test_datasetABC = SpeechDataset(classes_ABC, testing_listABC, classes_X, testing_listX)

# Create a DataLoader
test_dlABC = torch.utils.data.DataLoader(test_datasetABC, batch_size=1)

netA.eval()
netB.eval()
netC.eval()

test_loss = 0
test_accuracy = 0

preds, y_test = np.array([]), np.array([])
b1 = 10
#CUDA_LAUNCH_BLOCKING = 1
c = 0
for i, batch in enumerate(test_dlABC):

  with torch.no_grad():
    # Get a batch from the dataloader
    x = batch[0]
    labels = batch[1]

    #print(labels[0])
    # move the batch to GPU
    x = x.cuda()
    labels = labels.cuda()

    for i in range(0, 3):
      if (i == 0):
        y1 = netC(x)
        # Compute the loss
        if (y1.max(1)[1][0].cpu() == b1):
          continue
        #c += 1
        #print('model C', y1.max(1)[1], ' ', labels[0])
        
        #uncomment following condition if using test_dlABC else comment
        if (labels[0] < 20):
          labels[0] = 30
        labels[0] -= 20

        loss = criterion(y1, labels)
        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y1.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labelsA.cpu().numpy()])

        test_loss += loss.item()
        #print(y1.max(1))
        #print(y2.max(1))
        #break


        test_accuracy += (y1.max(1)[1] == labels).sum().item()
        break

      elif (i == 1):
        y2 = netB(x)
        #print('model B', y2.max(1)[1], ' ', labels[0])
        # Compute the loss
        if (y2.max(1)[1][0].cpu() == b1):
          continue
        #c += 1
        #print('model B', y1.max(1)[1], ' ', labels[0])
        
        #uncomment following condition if using test_dlABC else comment
        if (labels[0] < 10):
          labels[0] = 20
        labels[0] -= 10

        if (labels[0] > 10):
          labels[0] = 10

        loss = criterion(y2, labels)
        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y1.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labelsA.cpu().numpy()])

        test_loss += loss.item()
        #print(y1.max(1))
        #print(y2.max(1))
        #break


        test_accuracy += (y2.max(1)[1] == labels).sum().item()
        break

      else:
        y3 = netA(x)
        if (labels[0] > 10):
          labels[0] = 10
        loss = criterion(y3, labels)
        #print('model A', y3.max(1)[1], ' ', labels[0])
        
        

        ## Store all the predictions an labels for later
        #preds = np.hstack([preds, y2.max(1)[1].cpu().numpy()])
        #y_test = np.hstack([y_test, labels.cpu().numpy()])
        
        test_loss += loss.item()
        
        test_accuracy += (y3.max(1)[1] == labels).sum().item()
      
    #break
print("Test accuracy:", test_accuracy/float(len(test_datasetABC)),
      "Test loss:", test_loss/float(len(test_datasetABC)))
#print(c)
#print(len(test_datasetA))

